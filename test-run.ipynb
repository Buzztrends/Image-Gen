{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# environment setup\n",
    "with open(\".env\", \"r\") as key_file:\n",
    "    keys = list(key_file)\n",
    "\n",
    "for item in keys:\n",
    "    variable, value = item.split(\"=\")[0], \"=\".join(item.split(\"=\")[1:])\n",
    "    os.environ[variable] = value.replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import sagemaker, subprocess, boto3\n",
    "\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "from sagemaker import s3, get_execution_role\n",
    "from sagemaker.pytorch import PyTorchModel, PyTorchPredictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model 1: SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the right credentials, role and client for SageMaker\n",
    "sm_client = boto3.client(service_name=\"sagemaker\", region_name=\"ap-southeast-1\")\n",
    "role = os.environ[\"SAGEMAKER_ROLE\"]\n",
    "print(f'Role: {role}')\n",
    "\n",
    "INSTANCE_TYPE_SAM = 'ml.g4dn.xlarge'\n",
    "INSTANCE_TYPE_INPAINTING = 'ml.g4dn.2xlarge'\n",
    "\n",
    "bashCommand = \"tar -cpzf  code.tar.gz code/\"\n",
    "process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "s3_resource = boto3.resource('s3')\n",
    "sts = boto3.client('sts')\n",
    "AWS_ACCOUNT_ID = sts.get_caller_identity()[\"Account\"]\n",
    "REGION = s3_client.meta.region_name\n",
    "\n",
    "bucket = 'inpainting-test-s3'\n",
    "response = s3_client.list_buckets()\n",
    "for bucket in response['Buckets']:\n",
    "    if 'inpainting-test-s3' in bucket[\"Name\"]:\n",
    "        bucket = bucket[\"Name\"]\n",
    "        break\n",
    "print(f'Bucket: {bucket}')\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=bucket.split('s3://')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAM_ENDPOINT_NAME = 'sam-pytorch-' + str(datetime.utcnow().strftime('%Y-%m-%d-%H-%M-%S-%f'))\n",
    "%store SAM_ENDPOINT_NAME\n",
    "\n",
    "prefix_sam = \"SAM/demo-custom-endpoint\"\n",
    "\n",
    "model_data_sam = s3.S3Uploader.upload(\"code.tar.gz\", f's3://{bucket}/{prefix_sam}')\n",
    "print(f'Model Data: {model_data_sam}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sam = PyTorchModel(entry_point='inference_sam.py',\n",
    "                     model_data=model_data_sam, \n",
    "                     framework_version='1.12', \n",
    "                     py_version='py38',\n",
    "                     role=role,\n",
    "                     env={'TS_MAX_RESPONSE_SIZE':'2000000000', 'SAGEMAKER_MODEL_SERVER_TIMEOUT' : '300'},\n",
    "                     sagemaker_session=sess,\n",
    "                     name='model-'+SAM_ENDPOINT_NAME)\n",
    "\n",
    "print(f'SAM Endpoint Name: {SAM_ENDPOINT_NAME}')\n",
    "\n",
    "predictor_sam = model_sam.deploy(initial_instance_count=1, \n",
    "                         instance_type=INSTANCE_TYPE_SAM,\n",
    "                         deserializers=JSONDeserializer(),\n",
    "                         endpoint_name=SAM_ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPAINTING_ENDPOINT_NAME = 'inpainting-pytorch-' + str(datetime.utcnow().strftime('%Y-%m-%d-%H-%M-%S-%f'))\n",
    "%store INPAINTING_ENDPOINT_NAME\n",
    "\n",
    "prefix_inpainting = \"InPainting/demo-custom-endpoint\"\n",
    "\n",
    "model_data_inpainting = s3.S3Uploader.upload(\"code.tar.gz\", f\"s3://{bucket}/{prefix_inpainting}\")\n",
    "print(f'Model Data: {model_data_inpainting}')\n",
    "\n",
    "model_inpainting = PyTorchModel(entry_point='inference_inpainting.py',\n",
    "                     model_data=model_data_inpainting, \n",
    "                     framework_version='1.12', \n",
    "                     py_version='py38',\n",
    "                     role=role,\n",
    "                     env={'TS_MAX_RESPONSE_SIZE':'2000000000', 'SAGEMAKER_MODEL_SERVER_TIMEOUT' : '300'},\n",
    "                     sagemaker_session=sess,\n",
    "                     name='model-'+INPAINTING_ENDPOINT_NAME)\n",
    "\n",
    "print(f'InPainting Endpoint Name: {INPAINTING_ENDPOINT_NAME}')\n",
    "\n",
    "predictor_inpainting = model_inpainting.deploy(initial_instance_count=1, \n",
    "                         instance_type=INSTANCE_TYPE_INPAINTING,\n",
    "                         serializer=JSONSerializer(),\n",
    "                         deserializers=JSONDeserializer(),\n",
    "                         endpoint_name=INPAINTING_ENDPOINT_NAME,\n",
    "                        #  volume_size=128\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.list_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAM_ENDPOINT_NAME = \"inpainting-pytorch-2023-12-04-12-04-31-281536\"\n",
    "\n",
    "print(f'SAM Endpoint Name: {SAM_ENDPOINT_NAME}')\n",
    "\n",
    "raw_image = Image.open(\"images/speaker.png\").convert(\"RGB\")\n",
    "\n",
    "predictor_sam = PyTorchPredictor(endpoint_name=SAM_ENDPOINT_NAME,\n",
    "                             deserializer=JSONDeserializer())\n",
    "\n",
    "output_array = predictor_sam.predict(raw_image, initial_args={'Accept': 'application/json'})\n",
    "\n",
    "mask_image = Image.fromarray(np.array(output_array).astype(np.uint8))\n",
    "\n",
    "# save the image using PIL Image\n",
    "mask_image.save('images/speaker_mask.png')\n",
    "\n",
    "# We are going to plot the outputs\n",
    "plot_images = [raw_image, mask_image]\n",
    "titles = ['Original Product Image', 'Mask']\n",
    "fig, ax = plt.subplots(1,len(plot_images), dpi = 200)\n",
    "for k1, img in enumerate(plot_images):\n",
    "    ax[k1].imshow(img); ax[k1].axis('off')\n",
    "    ax[k1].set_title(titles[k1], fontsize=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_image = Image.open(\"images/speaker.png\").convert(\"RGB\")\n",
    "mask_image = Image.open('images/speaker_mask.png').convert('RGB')\n",
    "prompt_fr = \"apple, books\"\n",
    "prompt_bg = \"table\"\n",
    "negative_prompt = \"longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, letters\" \n",
    "\n",
    "inputs = {}\n",
    "inputs[\"image\"] = np.array(raw_image)\n",
    "inputs[\"mask\"] = np.array(mask_image)\n",
    "inputs[\"prompt_fr\"] = prompt_fr\n",
    "inputs[\"prompt_bg\"] = prompt_bg\n",
    "inputs[\"negative_prompt\"] = negative_prompt\n",
    "\n",
    "predictor_inpainting = PyTorchPredictor(endpoint_name=INPAINTING_ENDPOINT_NAME,\n",
    "                             serializer=JSONSerializer(),\n",
    "                             deserializer=JSONDeserializer())\n",
    "\n",
    "\n",
    "output_array = predictor_inpainting.predict(inputs, initial_args={'Accept': 'application/json'})\n",
    "\n",
    "gai_mask = Image.fromarray(np.array(output_array[2]).astype(np.uint8))\n",
    "gai_background = Image.fromarray(np.array(output_array[1]).astype(np.uint8))\n",
    "gai_image = Image.fromarray(np.array(output_array[0]).astype(np.uint8))\n",
    "post_image = Image.fromarray(np.array(output_array[3]).astype(np.uint8))\n",
    "\n",
    "# We are going to plot the outputs\n",
    "plot_images = [gai_mask, gai_background, gai_image, post_image]\n",
    "titles = ['Refined Mask', 'Generated Background', 'Generated Product Image', 'Post Process Image']\n",
    "fig, ax = plt.subplots(1,len(plot_images), dpi = 200)\n",
    "for k1, img in enumerate(plot_images):\n",
    "    ax[k1].imshow(img); ax[k1].axis('off')\n",
    "    ax[k1].set_title(titles[k1], fontsize=5)\n",
    "\n",
    "\n",
    "# save the generated image using PIL Image\n",
    "post_image.save('images/speaker_generated.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.list_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAM_ENDPOINT_NAME = \"sam-pytorch-2023-12-04-11-56-31-587907\"\n",
    "response = sm_client.describe_endpoint_config(EndpointConfigName=SAM_ENDPOINT_NAME)\n",
    "print(response)\n",
    "endpoint_config_name = response['EndpointConfigName']\n",
    "\n",
    "# Delete Endpoint\n",
    "sm_client.delete_endpoint(EndpointName=SAM_ENDPOINT_NAME)\n",
    "\n",
    "# Delete Endpoint Configuration\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "\n",
    "# Delete Model\n",
    "for prod_var in response['ProductionVariants']:\n",
    "    model_name = prod_var['ModelName']\n",
    "    sm_client.delete_model(ModelName=model_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
