{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75fccc56",
   "metadata": {},
   "source": [
    "# 2. Running Inference on the GenAI Models\n",
    "This Notebook is used to run inference on the 2 models hosted to SageMaker Endpoints. The first model is Segment Anything Model (SAM) and the second model is the InPainting model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fafeef",
   "metadata": {},
   "source": [
    "## 2.1 Install Dependencies and Import Libraries\n",
    "Here we are using `SageMaker, Numpy, Pillow` libraries for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c19e9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sagemaker.pytorch import PyTorchPredictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13551774",
   "metadata": {},
   "source": [
    "## 2.2 Model 1: SAM\n",
    "\n",
    "### Run inference on the SAM Model\n",
    "In this section, we will demonstrate the inference on the SAM model. \n",
    "The product image is provided as an input to the model and the product mask is genereted as an output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6196248a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r SAM_ENDPOINT_NAME\n",
    "print(f'SAM Endpoint Name: {SAM_ENDPOINT_NAME}')\n",
    "\n",
    "raw_image = Image.open(\"images/speaker.png\").convert(\"RGB\")\n",
    "\n",
    "predictor_sam = PyTorchPredictor(endpoint_name=SAM_ENDPOINT_NAME,\n",
    "                             deserializer=JSONDeserializer())\n",
    "\n",
    "output_array = predictor_sam.predict(raw_image, initial_args={'Accept': 'application/json'})\n",
    "\n",
    "mask_image = Image.fromarray(np.array(output_array).astype(np.uint8))\n",
    "\n",
    "# save the image using PIL Image\n",
    "mask_image.save('images/speaker_mask.png')\n",
    "\n",
    "# We are going to plot the outputs\n",
    "plot_images = [raw_image, mask_image]\n",
    "titles = ['Original Product Image', 'Mask']\n",
    "fig, ax = plt.subplots(1,len(plot_images), dpi = 200)\n",
    "for k1, img in enumerate(plot_images):\n",
    "    ax[k1].imshow(img); ax[k1].axis('off')\n",
    "    ax[k1].set_title(titles[k1], fontsize=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66cdc72",
   "metadata": {},
   "source": [
    "## 2.3 Model 2: InPainting\n",
    "\n",
    "### Run inference on the InPainting Model\n",
    "In this section, we will demonstrate the inference on the InPainting model.\n",
    "The product image, the previously generated mask, foreground promts, background prompts and negative prompts are used as an input to generate the right type of image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bcd460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r INPAINTING_ENDPOINT_NAME\n",
    "print(f'InPainting Endpoint Name: {INPAINTING_ENDPOINT_NAME}')\n",
    "\n",
    "raw_image = Image.open(\"images/speaker.png\").convert(\"RGB\")\n",
    "mask_image = Image.open('images/speaker_mask.png').convert('RGB')\n",
    "prompt_fr = \"apple, books\"\n",
    "prompt_bg = \"table\"\n",
    "negative_prompt = \"longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, letters\" \n",
    "\n",
    "inputs = {}\n",
    "inputs[\"image\"] = np.array(raw_image)\n",
    "inputs[\"mask\"] = np.array(mask_image)\n",
    "inputs[\"prompt_fr\"] = prompt_fr\n",
    "inputs[\"prompt_bg\"] = prompt_bg\n",
    "inputs[\"negative_prompt\"] = negative_prompt\n",
    "\n",
    "predictor_inpainting = PyTorchPredictor(endpoint_name=INPAINTING_ENDPOINT_NAME,\n",
    "                             serializer=JSONSerializer(),\n",
    "                             deserializer=JSONDeserializer())\n",
    "\n",
    "output_array = predictor_inpainting.predict(inputs, initial_args={'Accept': 'application/json'})\n",
    "\n",
    "gai_mask = Image.fromarray(np.array(output_array[2]).astype(np.uint8))\n",
    "gai_background = Image.fromarray(np.array(output_array[1]).astype(np.uint8))\n",
    "gai_image = Image.fromarray(np.array(output_array[0]).astype(np.uint8))\n",
    "post_image = Image.fromarray(np.array(output_array[3]).astype(np.uint8))\n",
    "\n",
    "# We are going to plot the outputs\n",
    "plot_images = [gai_mask, gai_background, gai_image, post_image]\n",
    "titles = ['Refined Mask', 'Generated Background', 'Generated Product Image', 'Post Process Image']\n",
    "fig, ax = plt.subplots(1,len(plot_images), dpi = 200)\n",
    "for k1, img in enumerate(plot_images):\n",
    "    ax[k1].imshow(img); ax[k1].axis('off')\n",
    "    ax[k1].set_title(titles[k1], fontsize=5)\n",
    "\n",
    "\n",
    "# save the generated image using PIL Image\n",
    "post_image.save('images/speaker_generated.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ac3bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516ce65a-319a-4cac-a4bb-de499541bca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
